{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "399ff4f8",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "How might we predict a country’s **protein supply per capita** through the analysis of social, environmental, and economic factors such as **GDP, temperature change, consumer prices, import and export quantity and production quantity**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f8aa9f",
   "metadata": {},
   "source": [
    "# Overall 7 steps Methodology for ML Project\n",
    "Reference: [Google's 7 steps of ML](https://towardsdatascience.com/the-googles-7-steps-of-machine-learning-in-practice-a-tensorflow-example-for-structured-data-96ccbb707d77)\n",
    "\n",
    "Step 1: [Gathering Data](#step1)    \n",
    "Step 2: [Data Preparation](#step2)  \n",
    "Step 3: [Choosing a model](#step3)  \n",
    "Step 4: [Training](#step4)  \n",
    "Step 5: [Evaluation](#step5)  \n",
    "Step 6: [Parameter Tuning](#step6)  \n",
    "Step 7: [Prediction](#step7)  \n",
    "\n",
    "Step 0: [Model Improvements](#modelimp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fcda05",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "# Step 1: Gathering Data\n",
    "  \n",
    "Our data is sourced from FAOstats \\<insert link>  \n",
    "  \n",
    "Before selecting our predictor values, we first designed a persona for each of our statistics. This is done below in [Step 2: data preparation](#step2) through the use of graphs to map out potentially interesting characteristics of each country in relation to each predictor.  \n",
    "\n",
    "We have summarized our findings in the table below:  \n",
    "\\<Insert table of persona in markdown>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "698faf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9d1590",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "# Step 2: Data Preparation\n",
    "1. Aggregation of data by countries (e.g averaging temperature changes, summing production/consumption over the year and averaging by population)\n",
    "* Ensure all data able to be compared fairly\n",
    "* Saved as new csv file with new columns, Data_aggregated.csv\n",
    "\n",
    "2. Visualize data and relationships of protein supply against various X using seaborn and matplotlib using scatter plots\n",
    "* To check if there are any obvious relationships that can be seen\n",
    "* Persona Analysis by observing distribution of datas for each X\n",
    "\n",
    "3. Preprocessing - Drop Null Rows and Missing Data\n",
    "* Save as new csv file, Data_dropped.csv\n",
    "\n",
    "4. Normalize data with Z-normalization\n",
    "* Save as new csv, Data_norm.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fee207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_z(dfin):\n",
    "    dfout = pd.DataFrame()\n",
    "    mean = dfin.mean(axis=0) \n",
    "    std = dfin.std(axis=0)  \n",
    "    dfout = ((dfin - mean)/std)\n",
    "    return dfout\n",
    "\n",
    "def get_features_targets(df, feature_names, target_names):\n",
    "    df_feature = df[feature_names]\n",
    "    df_target = df[target_names]\n",
    "    return df_feature, df_target\n",
    "\n",
    "def prepare_feature(df_feature):\n",
    "    # this is to convert table of x independent variables to X matrix, hence first column is all ones as x_0=1\n",
    "    np_feature = df_feature.to_numpy()\n",
    "    np_m = np_feature.shape[0]\n",
    "    if np_feature.ndim == 1: \n",
    "        np_feature = np_feature.reshape(np_m, 1)\n",
    "    big_X = np.concatenate((np.ones((np_m, 1), dtype=float), np_feature), axis=1)\n",
    "    return big_X\n",
    "\n",
    "def prepare_target(df_target):\n",
    "    # this is for the y dependent variable to be predicted\n",
    "    np_target = df_target.to_numpy()\n",
    "    return np_target\n",
    "\n",
    "def predict(df_feature, beta):\n",
    "    norm_feature = normalize_z(df_feature) \n",
    "    np_feature = prepare_feature(norm_feature) \n",
    "    y_hat = calc_linear(np_feature, beta)  \n",
    "    return y_hat\n",
    "\n",
    "def split_data(df_feature, df_target, random_state=None, test_size=0.5):\n",
    "    # df.sample and df.drop can also do this \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    df_feature_rows = df_feature.shape[0]\n",
    "    df_target_rows = df_target.shape[0]\n",
    "    \n",
    "    df_feature_split = int(test_size * df_feature_rows)\n",
    "    df_target_split = int(test_size * df_target_rows)\n",
    "    \n",
    "    df_feature_rando = np.random.choice(df_feature_rows, size=df_feature_rows, replace=False)  \n",
    "    df_feature_test = df_feature.iloc[df_feature_rando[:df_feature_split]]   # split the randomized index and use to get values in the idx rows\n",
    "    df_feature_train = df_feature.iloc[df_feature_rando[df_feature_split:]]\n",
    "    \n",
    "    df_target_test = df_target.iloc[df_feature_rando[:df_target_split]]\n",
    "    df_target_train = df_target.iloc[df_feature_rando[df_target_split:]]\n",
    "    \n",
    "    return df_feature_train, df_feature_test, df_target_train, df_target_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8786c2",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "# Step 3: Choosing a model\n",
    "\n",
    "Our chosen model is that of multiple linear regression.   \n",
    "\n",
    "In our model, we hypothesize that the protein supplied to people in a country is represented by the function $\\hat{y}$ as follows:\n",
    "\n",
    "$$\\hat{y}(x) =  \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\ldots + \\hat{\\beta}_n x_n$$  \n",
    "\n",
    "#### <center> OR </center>  \n",
    "\n",
    "$$\\mathbf{\\hat{y}} = \\mathbf{X} \\times \\mathbf{\\hat{b}}$$\n",
    "\n",
    "The cost function of our model is the following, which is derived from the sum of squared errors, i.e $\\frac{1}{2}$MSE:\n",
    "$$J(\\hat{\\beta}_0, \\hat{\\beta}_1) = \\frac{1}{2m}\\Sigma^m_{i=1}\\left(\\hat{y}(x^i)-y^i\\right)^2$$\n",
    "\n",
    "Using calculus to minimize our cost J against $\\hat{\\beta}$, we can derive our gradient descent update equation as the following:\n",
    "$$ \\mathbf{\\hat{b}} = \\mathbf{\\hat{b}} - \\alpha\\frac{1}{m} \\mathbf{X}^T \\times (\\mathbf{X}\\times \\mathbf{\\hat{b}} - \\mathbf{y}) $$\n",
    "\n",
    "With this, we get a column vector of $\\hat{\\beta}$ for our multiple linear regression model which best fits our data.  \n",
    "  \n",
    "To check if we have sufficiently minimized our cost, we can plot our cost at iteration, J, against iteration number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f078bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_linear(X, beta):\n",
    "    return np.matmul(X, beta)\n",
    "\n",
    "def compute_cost(X, y, beta):\n",
    "    J = 0\n",
    "    m = X.shape[0]\n",
    "    error = calc_linear(X, beta) - y\n",
    "    error_sq = np.matmul(error.T, error)\n",
    "    J = (1/(2*m))*error_sq\n",
    "    J = J[0][0]\n",
    "    return J\n",
    "\n",
    "def gradient_descent(X, y, beta, alpha, num_iters):\n",
    "    m = X.shape[0] \n",
    "    J_storage = np.zeros((num_iters, 1))\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        beta = beta - (alpha / m) * np.matmul(X.T, (calc_linear(X, beta) - y))\n",
    "        J_storage[i] = compute_cost(X, y, beta)\n",
    "    return beta, J_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a96c5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f45d9f0",
   "metadata": {},
   "source": [
    "<a id='step4'></a>\n",
    "# Step 4: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b53241b",
   "metadata": {},
   "source": [
    "<a id='step5'></a>\n",
    "# Step 5: Evaluation\n",
    "\n",
    "Metrics wise there are 3 that works in linear regression (excluding their upgraded variations that work in multiple linear regression):\n",
    "- R squared (Linear Regression only) —> Adjusted R squared (Both Single and Multiple Linear Regression)\n",
    "- Mean Squared Error (MSE) —> Root Mean Squared Error (RMSE) (i.e just corrected for the squaring of error)\n",
    "- Mean Absolute Error (MAE)\n",
    "\n",
    "MSE, RMSE and MAE measure the error of the model against the actual data. All 3 metrics range from 0 to $\\infty$, and is negatively-oriented (i.e the lower it is the better).\n",
    "  \n",
    "## R-Squared\n",
    "R squared measures the fit of the model by explaining how much of a variation of a dependent variable is explainable by independent variable in regression model. R squared only works on single linear regression, as it increases or remains the same, making the model seem like a better fit than it actually is.\n",
    "\n",
    "Instead, we use **Adjusted R-Squared** which adjusts for degree of freedom of data, and is unaffected by irrelevant predictors. \n",
    "$$ R^2_{\\mathrm{adj}} = 1 - \\frac{\\mathrm{RSS}/(n-p)}{\\mathrm{TSS}/(n-1)} $$\n",
    "\n",
    "Adjusted R-Squared is positive-oriented (i.e higher is better fit), and ranges from 0 to 1.  \n",
    "\n",
    "## Mean Squared Error\n",
    "MSE and RMSE are the same in distribution, but RMSE is scaled down in value. \n",
    "\n",
    "Mean Squared Error = Mean of squared error = $ \\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y}(x^i)-y_i)^2 $  \n",
    "Root Mean Squared Error = $ \\sqrt{MSE} $\n",
    "  \n",
    "MSE and RMSE are sensitive to outliers. This is as it gives higher weight to larger errors (residuals), and is better used when large errors are particularly undesirable. \n",
    "* Since we desire to minimize RMSE, we want to over represent large errors.\n",
    "\n",
    "This is sensitivity is due to the squaring of the errors, which we can see mathematically from how:\n",
    "$$ (\\frac{b}{a})^2 > \\frac{b}{a} \\text{, where b is the large error and a is the small error}$$\n",
    "\n",
    "\n",
    "## Mean Absolute Error\n",
    "MAE = Mean of absolute error = $\\frac{1}{n}\\sum_{i=1}^{n}|\\hat{y}(x^i)-y_i|$\n",
    "MAE treats every error equally and just finds their average.  \n",
    "\n",
    "However, absolute values are undesirable in mathematics as absolute value is a piecewise function which are subject to both algebra and a geometric interpretation of piecewise graph.\n",
    "\n",
    "$$ \\lvert x \\rvert = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            -x & \\quad x < 0 \\\\\n",
    "            x & \\quad x \\geq 0\n",
    "        \\end{array}\n",
    "    \\right. $$\n",
    "\n",
    "## Choice of Metric: RMSE\n",
    "Given our model of protein supply is worse off having larger errors due to outliers in the data, as well as the ease of mathematical calculations, we will be choosing RMSE.\n",
    "\n",
    "\n",
    "References: \n",
    "* [Medium: RMSE vs MAE - Which metric is better](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d#:~:text=RMSE%20has%20the%20benefit%20of,then%20MAE%20is%20more%20appropriate.)\n",
    "* [Medium: Comparing robustness of MAE, MSE and RMSE](https://towardsdatascience.com/comparing-robustness-of-mae-mse-and-rmse-6d69da870828)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d6fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score(y, ypred):\n",
    "    y_mean = y.mean()\n",
    "    ss_res = np.sum((y - ypred)**2)\n",
    "    ss_tot = np.sum((y - y_mean)**2)\n",
    "    return 1 - ss_res/ss_tot\n",
    "\n",
    "\n",
    "\n",
    "def mean_squared_error(target, pred):\n",
    "    n = target.shape[0]\n",
    "    rss = np.sum((target - pred)**2)\n",
    "    mse = rss / n\n",
    "    return mse\n",
    "\n",
    "def root_mean_squared_error(target, pred):\n",
    "    return sqrt(mean_squared_error(target, pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463ef4c8",
   "metadata": {},
   "source": [
    "<a id='step6'></a>\n",
    "# Step 6: Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1f6c47",
   "metadata": {},
   "source": [
    "<a id='step7'></a>\n",
    "# Step 7: Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80a0ed0",
   "metadata": {},
   "source": [
    "<a id='modelimp'></a>\n",
    "# Model Improvements\n",
    "## Improvement 1: Removal of outliers\n",
    "From our data, we can see certain countries protein supply are far from the median.  \n",
    "Examples include: .... countries ....   \n",
    "  \n",
    "We can arbitarily define outliers as protein supply values less than (Q1 - 1.5 IQR) or protein supply values more than (Q3 + 1.5 IQR). The functions to find these are defined below.\n",
    "  \n",
    "This has two implications that allows us to get a better model:  \n",
    "1. Our chosen cost metric, RMSE, is more sensitive to extreme values. By removing outliers, we can lower our RMSE.\n",
    "2. Our cost function J is equal to \\frac{1}{2} MSE which like RMSE is sensitive to outliers.\n",
    "\n",
    "# Improvement 2: Removal of irrelevant / low correlation predictors  \n",
    "  \n",
    "Based on our graphs, we will be removing ________ as our predictor(s).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485583cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quartile_1(df, target):\n",
    "    return\n",
    "\n",
    "def quartile_3(df, target):\n",
    "    return\n",
    "\n",
    "def iqr(target):\n",
    "    return quartile_3(df, target) - quartile_1(df, target)\n",
    "\n",
    "def remove_outliers(df, target):\n",
    "    \"\"\" \n",
    "    Returns a new dataframe with the outliers row removed.  \n",
    "    \"\"\"\n",
    "    return df_new\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
